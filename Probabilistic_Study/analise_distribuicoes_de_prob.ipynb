{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H9XtdGEXQFI"
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DQpxqtAwbTVz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: total: 1.88 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install -U aeon --quiet\n",
    "%pip install aeon[all_extras] --quiet\n",
    "%pip install --upgrade numba --quiet\n",
    "%pip install -U sktime --quiet\n",
    "%pip install -U matplotlib --quiet\n",
    "\n",
    "# Importações\n",
    "import os.path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from aeon.transformations.collection.convolution_based import Rocket\n",
    "from aeon.classification.convolution_based import RocketClassifier\n",
    "from aeon.datasets import load_classification, load_from_tsfile\n",
    "from aeon.transformations.collection.convolution_based import Rocket as OriginalRocket\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0GP7Ezb02ld"
   },
   "source": [
    "# Datasets escolhidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xaCNVHB3Uju0"
   },
   "outputs": [],
   "source": [
    "CHOSEN_DATASETS = [\n",
    "    'ArrowHead',\n",
    "    'DistalPhalanxTW',\n",
    "    'ECG5000',\n",
    "    'ElectricDevices',\n",
    "   'FaceAll',\n",
    "   'FiftyWords',\n",
    "    'GunPoint',\n",
    "    'Ham',\n",
    "    'InlineSkate',\n",
    "    'Lightning2',\n",
    "    'OSULeaf',\n",
    "    'RefrigerationDevices',\n",
    "    'ShapeletSim',\n",
    "    'ShapesAll',\n",
    "    'SyntheticControl',\n",
    "    'TwoPatterns'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peTbvh8xzyHI"
   },
   "source": [
    "# Rocket Distribuição Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "vez1WRqqvH1h",
    "outputId": "9b77bcee-39b0-4628-9fc5-97fac56ee73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.780114                0.015195\n",
      "1        DistalPhalanxTW        0.704029                0.007828\n",
      "2                ECG5000        0.947333                0.000498\n",
      "3        ElectricDevices        0.668777                0.002483\n",
      "4                FaceAll        0.933160                0.002786\n",
      "5             FiftyWords        0.770286                0.004746\n",
      "6               GunPoint        0.993200                0.002516\n",
      "7                    Ham        0.687810                0.015307\n",
      "8            InlineSkate        0.392109                0.004698\n",
      "9             Lightning2        0.727541                0.022436\n",
      "10               OSULeaf        0.831074                0.008860\n",
      "11  RefrigerationDevices        0.508640                0.012656\n",
      "12           ShapeletSim        0.649444                0.025576\n",
      "13             ShapesAll        0.865033                0.004635\n",
      "14      SyntheticControl        0.998667                0.001782\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment1(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\") # type: ignore\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\") # type: ignore\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = Rocket() # type: ignore\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    output_dir = 'Normal'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment1(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_normal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcQgl17OJ10L"
   },
   "source": [
    "# T-Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket implementado (T-Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sobrescreve o método _generate_kernels\n",
    "class RocketTStudent(OriginalRocket):\n",
    "    def _generate_kernels(self, n_timepoints, num_kernels, n_channels, seed=None):\n",
    "        # Configuração do estado aleatório, se especificado\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "        lengths = np.random.choice(candidate_lengths, num_kernels).astype(np.int32)\n",
    "        \n",
    "        num_channel_indices = np.zeros(num_kernels, dtype=np.int32)\n",
    "        for i in range(num_kernels):\n",
    "            limit = min(n_channels, lengths[i])\n",
    "            num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))\n",
    "        \n",
    "        # Inicialização dos arrays para armazenar pesos, bias, etc.\n",
    "        channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)\n",
    "        weights = np.zeros(\n",
    "            np.int32(\n",
    "                np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        biases = np.zeros(num_kernels, dtype=np.float32)\n",
    "        dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "        paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "        a1 = 0  # índice para pesos\n",
    "        a2 = 0  # índice para channel_indices\n",
    "\n",
    "        for i in range(num_kernels):\n",
    "            _length = lengths[i]\n",
    "            _num_channel_indices = num_channel_indices[i]\n",
    "            \n",
    "            # Geração dos pesos com a distribuição t-Student\n",
    "            _weights = np.random.standard_t(df=1, size=_num_channel_indices * _length).astype(np.float32)\n",
    "            \n",
    "            # Centralização dos pesos\n",
    "            b1 = a1 + (_num_channel_indices * _length)\n",
    "            a3 = 0\n",
    "            for _ in range(_num_channel_indices):\n",
    "                b3 = a3 + _length\n",
    "                _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()\n",
    "                a3 = b3\n",
    "            \n",
    "            weights[a1:b1] = _weights\n",
    "\n",
    "            # Definição dos índices dos canais e bias\n",
    "            channel_indices[a2:a2 + _num_channel_indices] = np.random.choice(\n",
    "                np.arange(0, n_channels), _num_channel_indices, replace=False\n",
    "            )\n",
    "            biases[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "            # Dilação e padding para o kernel\n",
    "            dilation = 2 ** np.random.uniform(\n",
    "                0, np.log2((n_timepoints - 1) / (_length - 1))\n",
    "            )\n",
    "            dilations[i] = np.int32(dilation)\n",
    "\n",
    "            padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "            paddings[i] = padding\n",
    "\n",
    "            # Atualiza índices\n",
    "            a1 = b1\n",
    "            a2 += _num_channel_indices\n",
    "\n",
    "        # Retorna os parâmetros dos kernels modificados\n",
    "        return (\n",
    "            weights,\n",
    "            lengths,\n",
    "            biases,\n",
    "            dilations,\n",
    "            paddings,\n",
    "            num_channel_indices,\n",
    "            channel_indices,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_HjG72cAJ9w2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.782514                0.015203\n",
      "1        DistalPhalanxTW        0.703741                0.006769\n",
      "2                ECG5000        0.947151                0.000756\n",
      "3        ElectricDevices        0.669496                0.002519\n",
      "4                FaceAll        0.933657                0.003189\n",
      "5             FiftyWords        0.769714                0.005879\n",
      "6               GunPoint        0.992267                0.002812\n",
      "7                    Ham        0.687048                0.010183\n",
      "8            InlineSkate        0.392873                0.006232\n",
      "9             Lightning2        0.733115                0.026710\n",
      "10               OSULeaf        0.829669                0.012249\n",
      "11  RefrigerationDevices        0.506133                0.012391\n",
      "12           ShapeletSim        0.647111                0.036339\n",
      "13             ShapesAll        0.865467                0.004260\n",
      "14      SyntheticControl        0.999133                0.001623\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment2(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = RocketTStudent()\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    output_dir = 'T-Student'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment2(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_tstudent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQGyxLkNOVEI"
   },
   "source": [
    "# Uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket Uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sy4iEEliOXLz"
   },
   "outputs": [],
   "source": [
    "#Sobrescreve o método _generate_kernels\n",
    "class RocketUniform(OriginalRocket):\n",
    "    def _generate_kernels(n_timepoints, num_kernels, n_channels, seed):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "        lengths = np.random.choice(candidate_lengths, num_kernels).astype(np.int32)\n",
    "\n",
    "        num_channel_indices = np.zeros(num_kernels, dtype=np.int32)\n",
    "        for i in range(num_kernels):\n",
    "            limit = min(n_channels, lengths[i])\n",
    "            num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))\n",
    "\n",
    "        channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)\n",
    "\n",
    "        weights = np.zeros(\n",
    "            np.int32(\n",
    "                np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        biases = np.zeros(num_kernels, dtype=np.float32)\n",
    "        dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "        paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "        a1 = 0  # for weights\n",
    "        a2 = 0  # for channel_indices\n",
    "\n",
    "        for i in range(num_kernels):\n",
    "            _length = lengths[i]\n",
    "            _num_channel_indices = num_channel_indices[i]\n",
    "\n",
    "            #_weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(np.float32) #utiliza a normal\n",
    "            _weights = np.random.uniform(-1, 1, _num_channel_indices * _length).astype(np.float32)  # distribuição uniforme entre -1 e 1\n",
    "\n",
    "            b1 = a1 + (_num_channel_indices * _length)\n",
    "            b2 = a2 + _num_channel_indices\n",
    "\n",
    "            a3 = 0  # for weights (per channel)\n",
    "            for _ in range(_num_channel_indices):\n",
    "                b3 = a3 + _length\n",
    "                _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()\n",
    "                a3 = b3\n",
    "\n",
    "            weights[a1:b1] = _weights\n",
    "\n",
    "            channel_indices[a2:b2] = np.random.choice(\n",
    "                np.arange(0, n_channels), _num_channel_indices, replace=False\n",
    "            )\n",
    "\n",
    "            biases[i] = np.random.uniform(-1, 1)#uniform distribution\n",
    "\n",
    "            dilation = 2 ** np.random.uniform(\n",
    "                0, np.log2((n_timepoints - 1) / (_length - 1))\n",
    "            )\n",
    "            dilation = np.int32(dilation)\n",
    "            dilations[i] = dilation\n",
    "\n",
    "            padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "            paddings[i] = padding\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "        return (\n",
    "            weights,\n",
    "            lengths,\n",
    "            biases,\n",
    "            dilations,\n",
    "            paddings,\n",
    "            num_channel_indices,\n",
    "            channel_indices,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FKrFsRUtOis7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.778514                0.013687\n",
      "1        DistalPhalanxTW        0.702878                0.006037\n",
      "2                ECG5000        0.947240                0.000677\n",
      "3        ElectricDevices        0.668515                0.002748\n",
      "4                FaceAll        0.933065                0.002701\n",
      "5             FiftyWords        0.770549                0.004984\n",
      "6               GunPoint        0.993600                0.003000\n",
      "7                    Ham        0.687048                0.014655\n",
      "8            InlineSkate        0.393527                0.004985\n",
      "9             Lightning2        0.733115                0.022956\n",
      "10               OSULeaf        0.831157                0.010821\n",
      "11  RefrigerationDevices        0.505280                0.012786\n",
      "12           ShapeletSim        0.648333                0.028731\n",
      "13             ShapesAll        0.865933                0.003842\n",
      "14      SyntheticControl        0.998400                0.002047\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment3(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = RocketUniform()\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    output_dir = 'Uniforme'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment3(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_uniforme.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nyjkfq1YOo5Q"
   },
   "source": [
    "# Exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFJSLLzrO1EQ"
   },
   "outputs": [],
   "source": [
    "#Sobrescreve o método _generate_kernels\n",
    "class RocketExponential(OriginalRocket):\n",
    "    def _generate_kernels(n_timepoints, num_kernels, n_channels, seed):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "        lengths = np.random.choice(candidate_lengths, num_kernels).astype(np.int32)\n",
    "\n",
    "        num_channel_indices = np.zeros(num_kernels, dtype=np.int32)\n",
    "        for i in range(num_kernels):\n",
    "            limit = min(n_channels, lengths[i])\n",
    "            num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))\n",
    "\n",
    "        channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)\n",
    "\n",
    "        weights = np.zeros(\n",
    "            np.int32(\n",
    "                np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        biases = np.zeros(num_kernels, dtype=np.float32)\n",
    "        dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "        paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "        a1 = 0  # for weights\n",
    "        a2 = 0  # for channel_indices\n",
    "\n",
    "        for i in range(num_kernels):\n",
    "            _length = lengths[i]\n",
    "            _num_channel_indices = num_channel_indices[i]\n",
    "\n",
    "            #_weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(np.float32) #utiliza a normal\n",
    "            _weights = np.random.exponential(scale=1, size=_num_channel_indices * _length).astype(np.float32)\n",
    "\n",
    "            b1 = a1 + (_num_channel_indices * _length)\n",
    "            b2 = a2 + _num_channel_indices\n",
    "\n",
    "            a3 = 0  # for weights (per channel)\n",
    "            for _ in range(_num_channel_indices):\n",
    "                b3 = a3 + _length\n",
    "                _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()\n",
    "                a3 = b3\n",
    "\n",
    "            weights[a1:b1] = _weights\n",
    "\n",
    "            channel_indices[a2:b2] = np.random.choice(\n",
    "                np.arange(0, n_channels), _num_channel_indices, replace=False\n",
    "            )\n",
    "\n",
    "            biases[i] = np.random.uniform(-1, 1)#uniform distribution\n",
    "\n",
    "            dilation = 2 ** np.random.uniform(\n",
    "                0, np.log2((n_timepoints - 1) / (_length - 1))\n",
    "            )\n",
    "            dilation = np.int32(dilation)\n",
    "            dilations[i] = dilation\n",
    "\n",
    "            padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "            paddings[i] = padding\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "        return (\n",
    "            weights,\n",
    "            lengths,\n",
    "            biases,\n",
    "            dilations,\n",
    "            paddings,\n",
    "            num_channel_indices,\n",
    "            channel_indices,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nw8Xqxr8Ot8v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.778971                0.014207\n",
      "1        DistalPhalanxTW        0.704748                0.008087\n",
      "2                ECG5000        0.947004                0.000686\n",
      "3        ElectricDevices        0.669498                0.002907\n",
      "4                FaceAll        0.934083                0.002567\n",
      "5             FiftyWords        0.770154                0.005122\n",
      "6               GunPoint        0.993333                0.002694\n",
      "7                    Ham        0.688762                0.013112\n",
      "8            InlineSkate        0.393018                0.004113\n",
      "9             Lightning2        0.728852                0.024831\n",
      "10               OSULeaf        0.827190                0.010875\n",
      "11  RefrigerationDevices        0.503733                0.011583\n",
      "12           ShapeletSim        0.645333                0.031753\n",
      "13             ShapesAll        0.865200                0.004945\n",
      "14      SyntheticControl        0.999133                0.001623\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment4(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = RocketExponential()\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    output_dir = 'Exponencial'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment4(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_exponencial.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Mi0OhUPm35"
   },
   "source": [
    "# Cauchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket Cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r8wszmTrO-oE"
   },
   "outputs": [],
   "source": [
    "#Sobrescreve o método _generate_kernels\n",
    "class RocketCauchy(OriginalRocket):\n",
    "    def _generate_kernels(n_timepoints, num_kernels, n_channels, seed):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "        lengths = np.random.choice(candidate_lengths, num_kernels).astype(np.int32)\n",
    "\n",
    "        num_channel_indices = np.zeros(num_kernels, dtype=np.int32)\n",
    "        for i in range(num_kernels):\n",
    "            limit = min(n_channels, lengths[i])\n",
    "            num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))\n",
    "\n",
    "        channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)\n",
    "\n",
    "        weights = np.zeros(\n",
    "            np.int32(\n",
    "                np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        biases = np.zeros(num_kernels, dtype=np.float32)\n",
    "        dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "        paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "        a1 = 0  # for weights\n",
    "        a2 = 0  # for channel_indices\n",
    "\n",
    "        for i in range(num_kernels):\n",
    "            _length = lengths[i]\n",
    "            _num_channel_indices = num_channel_indices[i]\n",
    "\n",
    "            #_weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(np.float32) #utiliza a normal\n",
    "            _weights = np.random.standard_cauchy(size=_num_channel_indices * _length).astype(np.float32)\n",
    "\n",
    "            b1 = a1 + (_num_channel_indices * _length)\n",
    "            b2 = a2 + _num_channel_indices\n",
    "\n",
    "            a3 = 0  # for weights (per channel)\n",
    "            for _ in range(_num_channel_indices):\n",
    "                b3 = a3 + _length\n",
    "                _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()\n",
    "                a3 = b3\n",
    "\n",
    "            weights[a1:b1] = _weights\n",
    "\n",
    "            channel_indices[a2:b2] = np.random.choice(\n",
    "                np.arange(0, n_channels), _num_channel_indices, replace=False\n",
    "            )\n",
    "\n",
    "            biases[i] = np.random.uniform(-1, 1)#uniform distribution\n",
    "\n",
    "            dilation = 2 ** np.random.uniform(\n",
    "                0, np.log2((n_timepoints - 1) / (_length - 1))\n",
    "            )\n",
    "            dilation = np.int32(dilation)\n",
    "            dilations[i] = dilation\n",
    "\n",
    "            padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "            paddings[i] = padding\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "        return (\n",
    "            weights,\n",
    "            lengths,\n",
    "            biases,\n",
    "            dilations,\n",
    "            paddings,\n",
    "            num_channel_indices,\n",
    "            channel_indices,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3rNJyWg-PsrW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.781943                0.012770\n",
      "1        DistalPhalanxTW        0.705180                0.006893\n",
      "2                ECG5000        0.947213                0.000762\n",
      "3        ElectricDevices        0.669545                0.002818\n",
      "4                FaceAll        0.934426                0.004034\n",
      "5             FiftyWords        0.769363                0.005317\n",
      "6               GunPoint        0.993600                0.003000\n",
      "7                    Ham        0.683619                0.012213\n",
      "8            InlineSkate        0.392800                0.005654\n",
      "9             Lightning2        0.733770                0.022841\n",
      "10               OSULeaf        0.830992                0.010061\n",
      "11  RefrigerationDevices        0.505973                0.012176\n",
      "12           ShapeletSim        0.648222                0.026831\n",
      "13             ShapesAll        0.866400                0.004356\n",
      "14      SyntheticControl        0.998867                0.002088\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment5(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = RocketCauchy()\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    output_dir = 'Cauchy'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment5(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_cauchy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vyxk63hSwRw"
   },
   "source": [
    "# LogNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket LogNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rdmj0MmxSxwI"
   },
   "outputs": [],
   "source": [
    "#Sobrescreve o método _generate_kernels\n",
    "class RocketLogNormal(OriginalRocket):\n",
    "    def _generate_kernels(n_timepoints, num_kernels, n_channels, seed):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "        lengths = np.random.choice(candidate_lengths, num_kernels).astype(np.int32)\n",
    "\n",
    "        num_channel_indices = np.zeros(num_kernels, dtype=np.int32)\n",
    "        for i in range(num_kernels):\n",
    "            limit = min(n_channels, lengths[i])\n",
    "            num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))\n",
    "\n",
    "        channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)\n",
    "\n",
    "        weights = np.zeros(\n",
    "            np.int32(\n",
    "                np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        biases = np.zeros(num_kernels, dtype=np.float32)\n",
    "        dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "        paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "        a1 = 0  # for weights\n",
    "        a2 = 0  # for channel_indices\n",
    "\n",
    "        for i in range(num_kernels):\n",
    "            _length = lengths[i]\n",
    "            _num_channel_indices = num_channel_indices[i]\n",
    "\n",
    "            #_weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(np.float32) #utiliza a normal\n",
    "            _weights = np.random.lognormal(0, 1, _num_channel_indices * _length).astype(np.float32)\n",
    "\n",
    "            b1 = a1 + (_num_channel_indices * _length)\n",
    "            b2 = a2 + _num_channel_indices\n",
    "\n",
    "            a3 = 0  # for weights (per channel)\n",
    "            for _ in range(_num_channel_indices):\n",
    "                b3 = a3 + _length\n",
    "                _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()\n",
    "                a3 = b3\n",
    "\n",
    "            weights[a1:b1] = _weights\n",
    "\n",
    "            channel_indices[a2:b2] = np.random.choice(\n",
    "                np.arange(0, n_channels), _num_channel_indices, replace=False\n",
    "            )\n",
    "\n",
    "            biases[i] = np.random.uniform(-1, 1)#uniform distribution\n",
    "\n",
    "            dilation = 2 ** np.random.uniform(\n",
    "                0, np.log2((n_timepoints - 1) / (_length - 1))\n",
    "            )\n",
    "            dilation = np.int32(dilation)\n",
    "            dilations[i] = dilation\n",
    "\n",
    "            padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "            paddings[i] = padding\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "        return (\n",
    "            weights,\n",
    "            lengths,\n",
    "            biases,\n",
    "            dilations,\n",
    "            paddings,\n",
    "            num_channel_indices,\n",
    "            channel_indices,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando experimento para o dataset: ArrowHead\n",
      "Experimento para ArrowHead concluído.\n",
      "\n",
      "Executando experimento para o dataset: DistalPhalanxTW\n",
      "Experimento para DistalPhalanxTW concluído.\n",
      "\n",
      "Executando experimento para o dataset: ECG5000\n",
      "Experimento para ECG5000 concluído.\n",
      "\n",
      "Executando experimento para o dataset: ElectricDevices\n",
      "Experimento para ElectricDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: FaceAll\n",
      "Experimento para FaceAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: FiftyWords\n",
      "Experimento para FiftyWords concluído.\n",
      "\n",
      "Executando experimento para o dataset: GunPoint\n",
      "Experimento para GunPoint concluído.\n",
      "\n",
      "Executando experimento para o dataset: Ham\n",
      "Experimento para Ham concluído.\n",
      "\n",
      "Executando experimento para o dataset: InlineSkate\n",
      "Experimento para InlineSkate concluído.\n",
      "\n",
      "Executando experimento para o dataset: Lightning2\n",
      "Experimento para Lightning2 concluído.\n",
      "\n",
      "Executando experimento para o dataset: OSULeaf\n",
      "Experimento para OSULeaf concluído.\n",
      "\n",
      "Executando experimento para o dataset: RefrigerationDevices\n",
      "Experimento para RefrigerationDevices concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapeletSim\n",
      "Experimento para ShapeletSim concluído.\n",
      "\n",
      "Executando experimento para o dataset: ShapesAll\n",
      "Experimento para ShapesAll concluído.\n",
      "\n",
      "Executando experimento para o dataset: SyntheticControl\n",
      "Experimento para SyntheticControl concluído.\n",
      "\n",
      "Executando experimento para o dataset: TwoPatterns\n",
      "Experimento para TwoPatterns concluído.\n",
      "\n",
      "                 dataset  media_accuracy  desvio_padrao_accuracy\n",
      "0              ArrowHead        0.781029                0.011454\n",
      "1        DistalPhalanxTW        0.702590                0.006099\n",
      "2                ECG5000        0.947280                0.000507\n",
      "3        ElectricDevices        0.669275                0.002740\n",
      "4                FaceAll        0.933740                0.003492\n",
      "5             FiftyWords        0.771253                0.004370\n",
      "6               GunPoint        0.992667                0.003086\n",
      "7                    Ham        0.685333                0.009991\n",
      "8            InlineSkate        0.392800                0.004073\n",
      "9             Lightning2        0.731475                0.024313\n",
      "10               OSULeaf        0.828843                0.012014\n",
      "11  RefrigerationDevices        0.505333                0.013265\n",
      "12           ShapeletSim        0.653222                0.028285\n",
      "13             ShapesAll        0.865967                0.004834\n",
      "14      SyntheticControl        0.998600                0.001915\n",
      "15           TwoPatterns        1.000000                0.000000\n"
     ]
    }
   ],
   "source": [
    "# Função para rodar o experimento\n",
    "def run_experiment6(dataset_name, num_iteracoes=50):\n",
    "    # Carregar e transformar os dados de teste/treinamento\n",
    "    x_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    x_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Inicializar listas para armazenar os resultados de cada iteração\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_iteracoes):\n",
    "        rocket = RocketLogNormal()\n",
    "        rocket.fit(x_train)\n",
    "\n",
    "        # Transformação rocket nos dados de teste e treinamento\n",
    "        x_train_transform = rocket.transform(x_train)\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "\n",
    "        # Fit classificador\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_train_transform, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(x_test_transform)\n",
    "        # Calcular a acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Adicionar os resultados da iteração às listas\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    # Criar DataFrame com os resultados de cada iteração\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'iteration': np.arange(1, num_iteracoes + 1),\n",
    "        'accuracy': accuracy_list,\n",
    "    })\n",
    "\n",
    "    # Calcular médias das métricas excluindo a coluna 'iteration'\n",
    "    medias = df_resultados.drop(columns=['iteration']).mean()\n",
    "    desvios_padrao = df_resultados.drop(columns=['iteration']).std()\n",
    "\n",
    "    # Criar a pasta \"TLogNormal\"\n",
    "    output_dir = 'LogNormal'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Definir caminhos completos para os arquivos CSV dentro da pasta \"TESTE\"\n",
    "    csv_filename = os.path.join(output_dir, f'{dataset_name}_resultados.csv')\n",
    "    medias_filename = os.path.join(output_dir, f'{dataset_name}_medias.csv')\n",
    "    desvios_padrao_filename = os.path.join(output_dir, f'{dataset_name}_DP.csv')\n",
    "\n",
    "    # Salvar resultados em arquivos CSV\n",
    "    df_resultados.to_csv(csv_filename, index=False)\n",
    "    medias.to_csv(medias_filename, index=True, header=True)\n",
    "    desvios_padrao.to_csv(desvios_padrao_filename, index=True, header=True)\n",
    "\n",
    "    return df_resultados, medias, desvios_padrao\n",
    "\n",
    "# Lista para armazenar os resultados de todos os datasets\n",
    "resultados_gerais = []\n",
    "\n",
    "# Executar o experimento para todos os datasets da lista CHOSEN_DATASETS\n",
    "for dataset in CHOSEN_DATASETS:\n",
    "    print(f\"Executando experimento para o dataset: {dataset}\")\n",
    "    resultados_experimento, medias, desvios_padrao = run_experiment6(dataset_name=dataset, num_iteracoes=50)\n",
    "    print(f\"Experimento para {dataset} concluído.\\n\")\n",
    "\n",
    "    # Armazenar os resultados do dataset atual\n",
    "    resultados_gerais.append({\n",
    "        'dataset': dataset,\n",
    "        'media_accuracy': medias['accuracy'],\n",
    "        'desvio_padrao_accuracy': desvios_padrao['accuracy']\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(resultados_gerais)\n",
    "\n",
    "print(df_final)\n",
    "df_final.to_csv(\"resultados_lognormal.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xXNu8K_Z0d5S",
    "JcQgl17OJ10L",
    "TQGyxLkNOVEI",
    "Nyjkfq1YOo5Q",
    "C5Mi0OhUPm35",
    "3Vyxk63hSwRw"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
